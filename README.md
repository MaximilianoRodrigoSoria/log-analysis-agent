# ğŸ“Š Log Analyzer

![Banner](docs/imgs/banner.gif)

[![Documentation](https://img.shields.io/badge/docs-available-blue.svg)](ARCHITECTURE.md)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](docker-compose.yml)
[![Python](https://img.shields.io/badge/python-3.7+-green.svg)](https://www.python.org/)
[![Flask](https://img.shields.io/badge/tech-Flask-green.svg)](https://flask.palletsprojects.com/)
[![Ollama](https://img.shields.io/badge/AI-Ollama-orange.svg)](https://ollama.ai/)
[![Architecture](https://img.shields.io/badge/architecture-Hexagonal-blue.svg)](ARCHITECTURE.md)
[![License](https://img.shields.io/badge/license-Open_Source-blue.svg)](LICENSE)
[![Language](https://img.shields.io/badge/language-Spanish-red.svg)](README.md)
[![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)

Sistema profesional de anÃ¡lisis de logs con generaciÃ³n automÃ¡tica de reportes usando LLM local (Ollama).

Proyecto refactorizado con **arquitectura hexagonal (Ports & Adapters)** para mÃ¡xima mantenibilidad y extensibilidad.

---

## ğŸ—ï¸ Arquitectura

```
log_analyzer/
â”œâ”€â”€ app/                    # Entrypoints (CLI y API)
â”‚   â”œâ”€â”€ cli.py             # Interfaz de lÃ­nea de comandos
â”‚   â””â”€â”€ api.py             # API REST con Flask
â”‚
â”œâ”€â”€ src/                   # Core del dominio
â”‚   â”œâ”€â”€ domain/           # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ model.py      # Entidades y objetos de valor
â”‚   â”‚   â””â”€â”€ use_cases.py  # Caso de uso: GenerateReportUseCase
â”‚   â”‚
â”‚   â”œâ”€â”€ ports/            # Interfaces (ABC)
â”‚   â”‚   â”œâ”€â”€ llm_port.py
â”‚   â”‚   â”œâ”€â”€ log_reader_port.py
â”‚   â”‚   â”œâ”€â”€ analyzer_port.py
â”‚   â”‚   â””â”€â”€ report_writer_port.py
â”‚   â”‚
â”‚   â”œâ”€â”€ adapters/         # Implementaciones
â”‚   â”‚   â”œâ”€â”€ llm_ollama.py
â”‚   â”‚   â”œâ”€â”€ log_reader_fs.py
â”‚   â”‚   â”œâ”€â”€ analyzer_regex.py
â”‚   â”‚   â””â”€â”€ report_writer_fs.py
â”‚   â”‚
â”‚   â””â”€â”€ config/           # ConfiguraciÃ³n centralizada
â”‚       â”œâ”€â”€ settings.py   # Variables de entorno
â”‚       â”œâ”€â”€ constants.py  # Constantes del proyecto
â”‚       â””â”€â”€ logging_config.py
â”‚
â”œâ”€â”€ datasets/             # Logs de ejemplo
â”œâ”€â”€ out/                  # Outputs (generados en runtime)
â”‚   â”œâ”€â”€ reports/         # Reportes Markdown
â”‚   â””â”€â”€ analysis/        # AnÃ¡lisis JSON
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

### Principios de Arquitectura

- **Hexagonal (Ports & Adapters)**: Dominio independiente de infraestructura
- **Dependency Inversion**: Domain no importa adapters
- **Single Responsibility**: Cada componente tiene una responsabilidad clara
- **Open/Closed**: Extensible sin modificar cÃ³digo existente (nuevos adapters)

---

## ğŸš€ InstalaciÃ³n

### Pre-requisitos

1. **Python 3.7+**
2. **Ollama** corriendo localmente
   ```bash
   # Instalar Ollama: https://ollama.ai
   ollama pull mistral
   ollama serve
   ```

### InstalaciÃ³n del proyecto

```bash
# Clonar o navegar al proyecto
cd log_analyzer

# Crear entorno virtual (recomendado)
python -m venv venv

# Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

---

## ğŸ“– Uso

### OpciÃ³n 1: CLI (LÃ­nea de comandos)

```bash
# Uso bÃ¡sico
python app/cli.py --input datasets/generated_logs.txt

# Especificar directorio de salida
python app/cli.py --input datasets/generated_logs.txt --output ./custom_out

# Con run_id personalizado
python app/cli.py --input datasets/generated_logs.txt --run-id mi-analisis-001

# Con nivel de logging
python app/cli.py --input datasets/generated_logs.txt --log-level DEBUG
```

**Salida esperada:**
```
[INFO] Log Analyzer CLI
[INFO] Archivo de entrada: datasets\generated_logs.txt
[INFO] Directorio de salida: out
[INFO] Modelo LLM: mistral

[INFO] Iniciando anÃ¡lisis...
[INFO] [run_id=abc123] Iniciando generaciÃ³n de reporte
[INFO] [run_id=abc123] Leyendo logs desde archivo: datasets\generated_logs.txt
[INFO] [run_id=abc123] Analizando estructura del log
[INFO] [run_id=abc123] AnÃ¡lisis completado: 10 eventos, 6 errores, 2 warnings
[INFO] [run_id=abc123] Generando reporte con LLM
[INFO] [run_id=abc123] Reporte generado exitosamente: out\reports\abc123.md

[OK] âœ… AnÃ¡lisis completado exitosamente!

Run ID: abc123
Reporte Markdown: C:\lab\log_analyzer\out\reports\abc123.md
AnÃ¡lisis JSON: C:\lab\log_analyzer\out\analysis\abc123.json

Resumen:
  - Total eventos: 10
  - Errores: 6
  - Warnings: 2
```

### OpciÃ³n 2: API REST

```bash
# Iniciar servidor
python app/api.py
```

El servidor iniciarÃ¡ en `http://localhost:5000`

#### Endpoints disponibles:

**GET /** - InformaciÃ³n de la API
```bash
curl http://localhost:5000/
```

**GET /health** - Health check
```bash
curl http://localhost:5000/health
```

**POST /analyze** - Analizar logs

```bash
# Ejemplo bÃ¡sico
curl -X POST http://localhost:5000/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "log_text": "2026-02-13 08:30:15 ERROR [main] com.example.service.UserService - Error al procesar\njava.lang.NullPointerException: Cannot invoke method\n\tat com.example.service.UserService.process(UserService.java:45)"
  }'

# Con run_id personalizado
curl -X POST http://localhost:5000/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "log_text": "...",
    "run_id": "custom-run-001"
  }'
```

**Respuesta exitosa:**
```json
{
  "status": "success",
  "run_id": "abc123def456",
  "report_path": "C:\\lab\\log_analyzer\\out\\reports\\abc123def456.md",
  "analysis_path": "C:\\lab\\log_analyzer\\out\\analysis\\abc123def456.json",
  "summary": {
    "total_events": 10,
    "total_errors": 6,
    "total_warnings": 2
  }
}
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

Todas las configuraciones se pueden sobrescribir con variables de entorno:

| Variable | Default | DescripciÃ³n |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | URL de Ollama |
| `OLLAMA_MODEL` | `mistral` | Modelo LLM a usar |
| `OUT_DIR` | `./out` | Directorio de salida |
| `LOG_LEVEL` | `INFO` | Nivel de logging (DEBUG, INFO, WARN, ERROR) |
| `REQUEST_TIMEOUT_SECONDS` | `120` | Timeout para requests HTTP |

**Ejemplo:**
```bash
# Windows CMD
set OLLAMA_MODEL=llama2
set LOG_LEVEL=DEBUG
python app/cli.py --input datasets/generated_logs.txt

# Linux/Mac
export OLLAMA_MODEL=llama2
export LOG_LEVEL=DEBUG
python app/cli.py --input datasets/generated_logs.txt
```

---

## ğŸ“ Outputs

El sistema genera dos tipos de archivos en `out/`:

### 1. AnÃ¡lisis JSON (`out/analysis/<run_id>.json`)

AnÃ¡lisis estructurado determinista del log:
```json
{
  "summary": {
    "total_events": 10,
    "total_errors": 6,
    "total_warnings": 2
  },
  "error_groups": [
    {
      "exception": "NullPointerException",
      "count": 2,
      "top_frame": {
        "where": "com.example.service.UserService.process",
        "file": "UserService.java",
        "line": 45
      },
      "logger": "com.example.service.UserService",
      "samples": [...],
      "first_ts": "2026-02-13 08:30:15",
      "last_ts": "2026-02-13 08:35:17"
    }
  ],
  "warnings": [...],
  "events": [...]
}
```

### 2. Reporte Markdown (`out/reports/<run_id>.md`)

Reporte profesional generado por el LLM con:
- Resumen ejecutivo
- AnÃ¡lisis de patrones
- Detalle de grupos de errores
- Recomendaciones tÃ©cnicas
- Conclusiones

---

## ğŸ”’ Notas de Seguridad

### âš ï¸ API Sin AutenticaciÃ³n

La API REST **NO tiene autenticaciÃ³n implementada**. Consideraciones:

- âœ… **OK para desarrollo local**
- âœ… **OK para redes internas protegidas**
- âŒ **NO exponer en internet sin autenticaciÃ³n**
- âŒ **NO usar en producciÃ³n sin seguridad adicional**

**Para producciÃ³n, considerar:**
- API Keys / Bearer tokens
- OAuth2 / JWT
- Rate limiting
- Firewall / VPN
- HTTPS obligatorio

### âš ï¸ Prompt Injection

El sistema envÃ­a los logs directamente al LLM. Si los logs contienen contenido malicioso o instrucciones de prompt injection, podrÃ­an influir en la salida del reporte.

**Mitigaciones:**
- Validar/sanitizar logs antes de procesar
- Usar modelos locales (Ollama) para evitar fuga de datos
- Revisar outputs generados en entornos crÃ­ticos

---

## ğŸ¯ Ejemplos Avanzados

### Cambiar modelo LLM
```bash
set OLLAMA_MODEL=llama2
python app/cli.py --input datasets/generated_logs.txt
```

### Conectar a Ollama remoto
```bash
set OLLAMA_BASE_URL=http://192.168.1.100:11434
python app/cli.py --input datasets/generated_logs.txt
```

### Aumentar timeout para logs grandes
```bash
set REQUEST_TIMEOUT_SECONDS=300
python app/cli.py --input large_logs.txt
```

### Logging detallado
```bash
python app/cli.py --input datasets/generated_logs.txt --log-level DEBUG
```

---

## ğŸ”§ Extensibilidad

Gracias a la arquitectura hexagonal, puedes extender el sistema fÃ¡cilmente:

### Agregar nuevo adapter de LLM (ej: OpenAI)

1. Crear `src/adapters/llm_openai.py` implementando `LLMPort`
2. En `app/cli.py` o `app/api.py`, cambiar:
   ```python
   # De:
   llm = OllamaLLM()
   # A:
   llm = OpenAILLM()
   ```

### Agregar lectura desde S3

1. Crear `src/adapters/log_reader_s3.py` implementando `LogReaderPort`
2. Usar en el entrypoint que corresponda

### Agregar analyzer con ML

1. Crear `src/adapters/analyzer_ml.py` implementando `AnalyzerPort`
2. Reemplazar `RegexLogAnalyzer()` por `MLAnalyzer()`

**El dominio no cambia, solo los adapters.**

---

## ğŸ“‚ PolÃ­tica de .gitignore

El `.gitignore` incluye `out/` por defecto porque:

- âœ… Los reportes pueden contener informaciÃ³n sensible
- âœ… Son archivos generados (no fuente)
- âœ… Cada ejecuciÃ³n genera nuevos archivos (ruido en git)

**Si quieres versionar reportes especÃ­ficos:**
```bash
git add -f out/reports/importante.md
```

---

## ğŸ› Troubleshooting

### Error: "No se puede conectar a Ollama"
```bash
# Verifica que Ollama estÃ© corriendo
ollama serve

# Verifica el endpoint
curl http://localhost:11434/api/version
```

### Error: "Modelo no encontrado"
```bash
# Descarga el modelo
ollama pull mistral
```

### Error: "Timeout"
```bash
# Aumenta el timeout
set REQUEST_TIMEOUT_SECONDS=300
```

### Logs con formato diferente
El analyzer usa regex especÃ­ficos para logs tipo Java/Spring. Para otros formatos:
1. Crear un nuevo analyzer implementando `AnalyzerPort`
2. Reemplazar `RegexLogAnalyzer` en los entrypoints

---

## ğŸ¤ ContribuciÃ³n

Para agregar nuevas funcionalidades:

1. **Ports**: Define la interfaz (ABC) en `src/ports/`
2. **Adapters**: Implementa la interfaz en `src/adapters/`
3. **Use Cases**: Actualiza lÃ³gica de negocio en `src/domain/use_cases.py`
4. **Entrypoints**: Compone dependencias en `app/cli.py` o `app/api.py`

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto para fines educativos y de laboratorio.

---

## ğŸ‘¨â€ğŸ’» Arquitectura TÃ©cnica - Resumen

- **PatrÃ³n**: Hexagonal (Ports & Adapters)
- **Lenguaje**: Python 3.7+
- **LLM**: Ollama (local)
- **Framework API**: Flask
- **Testing**: Arquitectura permite fÃ¡cil testing con mocks de ports
- **Logging**: `logging` estÃ¡ndar con run_id tracking
- **Config**: Variables de entorno + defaults
- **Output**: JSON (anÃ¡lisis) + Markdown (reporte)

**Ventajas de esta arquitectura:**
- âœ… Dominio desacoplado de infraestructura
- âœ… FÃ¡cil testing (mock de ports)
- âœ… Extensible sin modificar dominio
- âœ… Mantenible a largo plazo
- âœ… Claro y documentado

---

## ğŸ“ Contacto

**Maximiliano Rodrigo Soria**

- ğŸ“± TelÃ©fono: +54 9 11 2704-3256 (Argentina)
- ğŸ’¼ GitHub: [MaximilianoRodrigoSoria](https://github.com/MaximilianoRodrigoSoria)

Para consultas, sugerencias o contribuciones al proyecto.

---

**Happy logging! ğŸ“ŠğŸš€**
